{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vbzALNyvRq4"
      },
      "source": [
        "# Introduction to Data Science 2025\n",
        "\n",
        "# Week 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf5yrH3ivRq8"
      },
      "source": [
        "## Exercise 1 | Working with geospatial data (GIS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8n5AKgIvRq8"
      },
      "source": [
        "To get at least a bit familiar with [GIS](https://en.wikipedia.org/wiki/Geographic_information_system) data and the concept of map projections, we’ll do a simple task of plotting two data sets that are given in different coordinate systems.\n",
        "\n",
        "1. Download the [world_m.zip](https://github.com/HY-TKTL/intro-to-data-science-2017/blob/master/world_m.zip) and [cities.zip](https://github.com/HY-TKTL/intro-to-data-science-2017/blob/master/cities.zip) files that each include a set of GIS files. Most notably, the <span style=\"font-weight: bold\">shp</span> files are [Shapefile-files](https://en.wikipedia.org/wiki/Shapefile) with coordinates (don’t look, it’s binary!). The <span style=\"font-weight: bold\">prj</span> files contain information (in plain text, so okay to look) about the coordinate systems. Open the files using your favorite programming environment and packages.  \n",
        "\n",
        "    <span style=\"font-weight: 500\"> *Hint: We warmly recommend [Geopandas](http://geopandas.org/) for pythonistas.*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNBmwJWIvRq9"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "world_gdf = gpd.read_file(\"assets/world_m/world_m.shp\")\n",
        "world_gdf\n",
        "\n",
        "cities_gdf = gpd.read_file(\"assets/cities/cities.shp\")\n",
        "cities_gdf\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDVMTE5OvRq9"
      },
      "source": [
        "2. The <span style=\"font-weight: bold\">world_m</span> file contains borders of almost all countries in the world. Plot the world."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6InHn-CBvRq-"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code\n",
        "\n",
        "world_gdf.plot(edgecolor=\"black\", linewidth=0.3).set_axis_off()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aelVUwlhvRq-"
      },
      "source": [
        "3. On top of the countries that you just plotted, plot another layer of information, namely the capital cities of each country from the <span style=\"font-weight: bold\">cities</span> dataset. However, depending on how clever your programming environment is, the cities will probably all appear to be in the Gulf of Guinea, near the coordinates (0°, 0°)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvfOO_ikvRq-"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code\n",
        "if world_gdf.crs != cities_gdf.crs:\n",
        "    cities_gdf = cities_gdf.to_crs(world_gdf.crs)\n",
        "    \n",
        "\n",
        "cities_gdf.plot(color=\"magenta\", markersize=4).set_axis_off()\n",
        "world_gdf.plot(edgecolor=\"black\", facecolor=\"grey\", linewidth=0.3).set_axis_off()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFGsoGT4vRq_"
      },
      "source": [
        "4. Perform a map projection to bring the two data-sets into a shared coordinate system. (You can choose which one.) Now plot the two layers together to make sure the capital cities are where they are supposed to be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyT4S1dAvRq_"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code\n",
        "if world_gdf.crs != cities_gdf.crs:\n",
        "    cities_gdf = cities_gdf.to_crs(world_gdf.crs)\n",
        "    \n",
        "\n",
        "ax = world_gdf.plot(edgecolor=\"black\", facecolor=\"grey\", linewidth=0.3)\n",
        "cities_gdf.plot(ax=ax, color=\"magenta\", markersize=4)\n",
        "ax.set_axis_off()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6xpgZlYvRq_"
      },
      "source": [
        "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szhdrpLKvRq_"
      },
      "source": [
        "## Exercise 2 | Symbol classification\n",
        "\n",
        "We’ll be looking into machine learning by checking out the [HASYv2](https://zenodo.org/record/259444#.Wb7efZ8xDhZ) dataset that contains hand written mathematical symbols as images. The whole dataset is quite big, so we’ll restrict ourselves to doing 10-class classification on some of the symbols. Download the data and complete the following tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0QQVXIPvRq_"
      },
      "source": [
        "1. Extract the data and find inside a file called <span style=\"font-weight: bold\">hasy-data-labels.csv</span>. This file contains the labels for each of the images in the <span style=\"font-weight: bold\">hasy_data</span> folder. Read the labels in and only keep the rows where the <span style=\"font-weight: bold\">symbol_id</span> is within the inclusive range <span style=\"font-weight: bold\">[70, 79]</span>. Read the corresponding images as black-and-white images and flatten them so that each image is a single vector of shape <span style=\"font-weight: bold\">32x32 = 1024</span>. Your dataset should now consist of your input data of shape <span style=\"font-weight: bold\">(1020, 1024)</span> and your labels of shape <span style=\"font-weight: bold\">(1020, )</span>. That is, a matrix of shape <span style=\"font-weight: bold\">1020 x 1024</span> and a vector of size <span style=\"font-weight: bold\">1020</span>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "QQkiE2ecvRq_"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>vec</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>70</td>\n",
              "      <td>[255, 255, 255, 255, 255, 255, 255, 255, 255, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>70</td>\n",
              "      <td>[255, 255, 255, 255, 255, 255, 255, 255, 255, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>70</td>\n",
              "      <td>[255, 255, 255, 255, 255, 255, 255, 255, 255, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>70</td>\n",
              "      <td>[255, 255, 255, 255, 255, 255, 255, 255, 255, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>70</td>\n",
              "      <td>[255, 255, 255, 255, 255, 255, 255, 255, 255, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1015</th>\n",
              "      <td>79</td>\n",
              "      <td>[255, 255, 255, 255, 255, 255, 255, 255, 255, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1016</th>\n",
              "      <td>79</td>\n",
              "      <td>[255, 255, 255, 255, 255, 255, 255, 255, 255, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1017</th>\n",
              "      <td>79</td>\n",
              "      <td>[255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1018</th>\n",
              "      <td>79</td>\n",
              "      <td>[255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1019</th>\n",
              "      <td>79</td>\n",
              "      <td>[255, 255, 255, 255, 255, 255, 255, 255, 255, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1020 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      label                                                vec\n",
              "0        70  [255, 255, 255, 255, 255, 255, 255, 255, 255, ...\n",
              "1        70  [255, 255, 255, 255, 255, 255, 255, 255, 255, ...\n",
              "2        70  [255, 255, 255, 255, 255, 255, 255, 255, 255, ...\n",
              "3        70  [255, 255, 255, 255, 255, 255, 255, 255, 255, ...\n",
              "4        70  [255, 255, 255, 255, 255, 255, 255, 255, 255, ...\n",
              "...     ...                                                ...\n",
              "1015     79  [255, 255, 255, 255, 255, 255, 255, 255, 255, ...\n",
              "1016     79  [255, 255, 255, 255, 255, 255, 255, 255, 255, ...\n",
              "1017     79  [255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, ...\n",
              "1018     79  [255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, ...\n",
              "1019     79  [255, 255, 255, 255, 255, 255, 255, 255, 255, ...\n",
              "\n",
              "[1020 rows x 2 columns]"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use this cell for your code\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# read labels\n",
        "labels_df = pd.read_csv(\"assets/hasy-data-labels.csv\")\n",
        "\n",
        "# in range\n",
        "labels_df = labels_df[(labels_df[\"symbol_id\"] >= 70) & (labels_df[\"symbol_id\"] <= 79)]\n",
        "\n",
        "rows = []\n",
        "\n",
        "for idx, item in labels_df.iterrows():\n",
        "    img = Image.open(f\"assets/{item[\"path\"]}\").convert(\"L\").resize((32,32))\n",
        "    vec = np.array(img).flatten()\n",
        "    rows.append({ \"label\": item[\"symbol_id\"], \"vec\": vec})\n",
        "    \n",
        "data_df = pd.DataFrame(rows)\n",
        "data_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sZQ5m5lvRrA"
      },
      "source": [
        "2. Randomly shuffle the data, and then split it into training and test sets, using the first 80% of the data for training and the rest for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "XJ7yRem-vRrA"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "\n",
        "# shuffle\n",
        "shuffled_df = data_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Split to training and test sets\n",
        "X = np.vstack(shuffled_df[\"vec\"].values)   # shape (n_samples, 1024), numeric 2D array\n",
        "y = shuffled_df[\"label\"].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu7qCqqpvRrA"
      },
      "source": [
        "3. Fit a logistic regression classifier on the data. Note that we have a multi-class classification problem, but logistic regression is a binary classifier. For this reason, you will find useful <span style=\"font-weight: bold\">[Sklearn's \"multi_class\" attribute](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)</span>. Use a multinomial loss and the softmax function to predict the probability of each class as the outcome.  The classifier should select the class with the highest probability. Most library implementations will do this for you - feel free to use one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "IalbaG5JvRrA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sutipong/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/home/sutipong/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/home/sutipong/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/home/sutipong/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/home/sutipong/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/home/sutipong/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on validation set: 85.88% (std = 0.02)\n"
          ]
        }
      ],
      "source": [
        "# Use this cell for your code\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "results = cross_val_score(model, X, y, scoring=\"accuracy\")\n",
        "print('Accuracy on validation set: %.2f%% (std = %.2f)' % (results.mean()*100, results.std()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YSdkvVqvRrA"
      },
      "source": [
        "4. In order to evaluate the model, let’s create our own dummy classifier that simply guesses the most common class in the training set (looking at the test set here would be cheating!). Then, evaluate your logistic regression model on the test data, and compare it to the majority class classifier. The logistic regression model should have significantly better accuracy as the dummy model is merely making a guess.\n",
        "\n",
        "    <span style=\"font-weight: 500\"> *Hint: Sklearn's DummyClassifier( ) might save you a bit of time.*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "3T-_cfYIvRrA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on validation set with LogisticRegression: 85.88% (std = 0.02)\n",
            "Accuracy on validation set with DummyClassifier: 13.04% (std = 0.00)\n"
          ]
        }
      ],
      "source": [
        "# Use this cell for your code\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "dmodel = DummyClassifier(random_state=42)\n",
        "dmodel.fit(X_train, y_train)\n",
        "\n",
        "dresults = cross_val_score(dmodel, X, y, scoring=\"accuracy\")\n",
        "\n",
        "print('Accuracy on validation set with LogisticRegression: %.2f%% (std = %.2f)' % (results.mean()*100, results.std()))\n",
        "print('Accuracy on validation set with DummyClassifier: %.2f%% (std = %.2f)' % (dresults.mean()*100, dresults.std()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59WsDIEZvRrA"
      },
      "source": [
        "5. Plot some of the images that the logistic classifier misclassified. Can you think of an explanation why they were misclassified? Would you have gotten them right?\n",
        "    \n",
        "    <span style=\"font-weight: 500\">*Hint: Matplotlib has a [function](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) that can help you with plotting.*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[4.81152746e-04 9.99518846e-01 7.21362462e-21 ... 1.23275032e-14\n",
            "  4.42500034e-18 1.14476760e-10]\n",
            " [1.00000000e+00 1.96842974e-26 3.27285427e-15 ... 1.61541587e-26\n",
            "  8.87478268e-26 3.23813841e-21]\n",
            " [1.79420512e-14 1.94242193e-21 1.93331419e-05 ... 8.88785588e-23\n",
            "  2.40039917e-12 2.27726352e-21]\n",
            " ...\n",
            " [1.65571187e-21 3.07436525e-22 5.77103005e-20 ... 3.11717462e-23\n",
            "  2.45393726e-09 3.12400794e-05]\n",
            " [1.16727017e-19 1.31124678e-18 9.15328471e-20 ... 1.54952234e-20\n",
            "  3.87166989e-10 2.14599493e-25]\n",
            " [1.74936455e-16 2.22800595e-12 2.27317350e-18 ... 1.00771088e-11\n",
            "  5.90205060e-04 3.66407718e-09]]\n",
            "Number of misclassified samples: 27\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1800x200 with 9 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "probs = model.predict_proba(X_test)\n",
        "\n",
        "print(probs)\n",
        "\n",
        "mis_idx = np.where(y_pred != y_test)[0]\n",
        "print(f\"Number of misclassified samples: {len(mis_idx)}\")\n",
        "\n",
        "n_show = min(9, len(mis_idx))\n",
        "\n",
        "cols = n_show\n",
        "fig, axes = plt.subplots(1, cols, figsize=(2*cols, 2))\n",
        "if cols == 1:\n",
        "    axes = [axes]\n",
        "for ax, idx in zip(axes, mis_idx[:n_show]):\n",
        "    ax.imshow(X_test[idx].reshape(32,32), cmap=\"gray_r\")\n",
        "    top_prob = probs[idx].max()\n",
        "    ax.set_title(f\"true:{y_test[idx]} pred:{y_pred[idx]}\")\n",
        "    ax.axis(\"off\")\n",
        "    \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QK_c3BtvRrA"
      },
      "source": [
        "Here are some examples of the syntax used to fit a logistic regression classifier (using Sklearn or statsmodel with Python, or GLM with R):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5j6uqp_xvRrA"
      },
      "outputs": [],
      "source": [
        "#Sklearn (python)\n",
        "\n",
        "#from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#Fit on the training data set, with:\n",
        "#X : {array-like, sparse matrix}, shape (n_samples, n_features) , n_samples rows x n_features columns\n",
        "#with attributes that describe each sample.\n",
        "#y : array-like, shape (n_samples,) , n_samples target values for each sample.\n",
        "\n",
        "#model = LogisticRegression()\n",
        "#model.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2bLFxM5vRrB"
      },
      "outputs": [],
      "source": [
        "#Statsmodels (python)\n",
        "\n",
        "#import statsmodels.api as sm\n",
        "#model = sm.Logit(y, X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAguOrDzvRrB"
      },
      "outputs": [],
      "source": [
        "#GLM (R)\n",
        "\n",
        "#model <- glm(y ~.,family=binomial(link='logit'), data=X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJuyAVmtvRrB"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-I7ERLhvRrB"
      },
      "source": [
        "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
